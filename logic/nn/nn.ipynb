{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chess\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use neural network to evaluate positions\n",
    "\n",
    "\n",
    "Notes:\n",
    "- input; how do we encode the board\n",
    "    - 12 by 64 input shape of bitboards\n",
    "\n",
    "\n",
    "Todo:\n",
    "- figure out number of layers\n",
    "- figure out which activation function to use\n",
    "\n",
    "Layers:\n",
    "- input layer\n",
    "    - represents state of the board\n",
    "    - maybe 12 bitboards?\n",
    "    - s = SAME_TEAM\n",
    "    - o = OTHER_TEAM\n",
    "    - input\n",
    "        - S PAWN\n",
    "        - S KNIGHT\n",
    "        - S BISHOP\n",
    "        - S ROOK\n",
    "        - S QUEEN\n",
    "        - S KING\n",
    "        - O PAWN\n",
    "        - O KNIGHT\n",
    "        - O BISHOP\n",
    "        - O ROOK\n",
    "        - O QUEEN\n",
    "        - O KING\n",
    "- \n",
    "- output layer\n",
    "    - one number that is the evalution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitboard(bb: int):\n",
    "    s = 8 * np.arange(7, -1, -1, dtype=np.uint64)\n",
    "    b = (bb >> s).astype(np.uint8)\n",
    "    b = np.unpackbits(b, bitorder=\"little\")\n",
    "    return b\n",
    "\n",
    "def bitboard_pretty(bb: np.array):\n",
    "    for i in range(0, 64):\n",
    "        if ((i) % 8 == 0):\n",
    "            print()\n",
    "        print(bb[i], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 0 0 "
     ]
    }
   ],
   "source": [
    "board = chess.Board()\n",
    "bitboard_pretty(bitboard(int(board.pieces(chess.PAWN, chess.WHITE))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nkeras sequntial model \\noptim: adam\\nloss: mse\\nModel architecture:\\n    later we could try a different type of input, but for now we will stick with this\\n    the problem with 12*64 inputs is that if we change one thing in the position, the input is completely different, and in a search we usually look at similar moves, so if we do not do this\\n    it is very bad for cache and it will run a lot slower.\\n\\n    linear of input 768 (12*64) bitboard\\n    we could also try a 64 input layer with:\\n    0.1 as pawn\\n    0.2 as knight\\n    0.3 as bishop\\n    0.4 as rook\\n    0.5 as queen\\n    1 as king\\n    this way if the one piece moves, only two inputs are changed\\n\\ni just made each layer 2/3 of the last layer plus the output\\nhttps://www.heatonresearch.com/2017/06/01/hidden-layers.html \\n\\nlinear layer: input_shape: 768; output_shape: 513; activation function: relu, tanh, or sigmoid\\nlinear layer: input_shape: 513; output_shape: 343; activation function: relu, tanh, or sigmoid\\nlinear layer: input_shape: 343; output_shape: 229; activation function: relu, tanh, or sigmoid\\nlinear layer: input_shape: 229; output_shape: 153; activation function: relu, tanh, or sigmoid\\nlinear layer: input_shape: 153; output_shape: 1\\n    \\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "keras sequntial model \n",
    "optim: adam\n",
    "loss: mse\n",
    "Model architecture:\n",
    "    linear of input 768 (12*64) bitboard\n",
    "\n",
    "    i just made each layer 2/3 of the last layer plus the output\n",
    "    https://www.heatonresearch.com/2017/06/01/hidden-layers.html \n",
    "\n",
    "    stockfish uses clamped relu(0, 1) for speed, but for now we can just use relu\n",
    "\n",
    "    linear layer: input_shape: 768; output_shape: 513; activation function: relu\n",
    "    linear layer: input_shape: 513; output_shape: 343; activation function: relu\n",
    "    linear layer: input_shape: 343; output_shape: 229; activation function: relu\n",
    "    linear layer: input_shape: 229; output_shape: 153; activation function: relu\n",
    "    linear layer: input_shape: 153; output_shape: 1\n",
    "    \n",
    "    In c++, we will not have to do multiplications in the nn because it is just 0s and 1s\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chess_Model(nn.Module):\n",
    "    def __init__(self, activation_func: function) -> None:\n",
    "        super().__init_()\n",
    "        self.acivation_function = activation_func\n",
    "        self.linear1 = nn.Linear(in_features=768, out_features=513)\n",
    "        self.linear2 = nn.Linear(in_features=513, out_features=343)\n",
    "        self.linear3 = nn.Linear(in_features=343, out_features=229)\n",
    "        self.linear4 = nn.Linear(in_features=229, out_features=153)\n",
    "        self.linear5 = nn.Linear(in_features=153, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.acivation_function(self.linear1(x))\n",
    "        x = self.acivation_function(self.linear2(x))\n",
    "        x = self.acivation_function(self.linear3(x))\n",
    "        x = self.acivation_function(self.linear4(x))\n",
    "        x = self.linear5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
